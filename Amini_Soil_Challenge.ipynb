{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQAiirxKi4dZ",
        "outputId": "4e86f61c-9f54-460e-ce9d-5c1ed1421f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages are installed!\n"
          ]
        }
      ],
      "source": [
        "# Check if required packages are installed\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    print(\"✅ All packages are installed!\")\n",
        "except ImportError as e:\n",
        "    print(\"❌ Missing package:\", e.name)\n"
      ]
    },
    {
      "source": [
        "# Check for missing values\n",
        "import numpy as np # Import numpy\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNPlEiRHCNq7",
        "outputId": "a4399c16-bd1e-4e52-ce00-50e6b5205758"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site           0\n",
            "pid            0\n",
            "lon            0\n",
            "lat            0\n",
            "ph             0\n",
            "alb            0\n",
            "bio1           0\n",
            "bio12          0\n",
            "bio15          0\n",
            "bio7           0\n",
            "bp             0\n",
            "cec20          0\n",
            "dows           0\n",
            "ecec20         0\n",
            "hp20           0\n",
            "ls             0\n",
            "lstd           0\n",
            "lstn           0\n",
            "mb1            0\n",
            "mb2            0\n",
            "mb3            0\n",
            "mb7            0\n",
            "mdem           0\n",
            "para           0\n",
            "parv           0\n",
            "ph20           0\n",
            "slope          0\n",
            "snd20          0\n",
            "soc20          0\n",
            "tim            0\n",
            "wp             0\n",
            "xhp20          0\n",
            "bulkdensity    0\n",
            "n              0\n",
            "p              0\n",
            "k              0\n",
            "ca             0\n",
            "mg             0\n",
            "s              0\n",
            "fe             0\n",
            "mn             0\n",
            "zn             0\n",
            "cu             0\n",
            "b              0\n",
            "dtype: int64\n",
            "site           0\n",
            "pid            0\n",
            "lon            0\n",
            "lat            0\n",
            "ph             0\n",
            "alb            0\n",
            "bio1           0\n",
            "bio12          0\n",
            "bio15          0\n",
            "bio7           0\n",
            "bp             0\n",
            "cec20          0\n",
            "dows           0\n",
            "ecec20         0\n",
            "hp20           0\n",
            "ls             0\n",
            "lstd           0\n",
            "lstn           0\n",
            "mb1            0\n",
            "mb2            0\n",
            "mb3            0\n",
            "mb7            0\n",
            "mdem           0\n",
            "para           0\n",
            "parv           0\n",
            "ph20           0\n",
            "slope          0\n",
            "snd20          0\n",
            "soc20          0\n",
            "tim            0\n",
            "wp             0\n",
            "xhp20          0\n",
            "bulkdensity    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-efae89c06618>:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
            "<ipython-input-12-efae89c06618>:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_test[col].fillna(df_test[col].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Check if required packages are installed\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    print(\"✅ All packages are installed!\")\n",
        "except ImportError as e:\n",
        "    print(\"❌ Missing package:\", e.name)\n",
        "\n",
        "# Load the datasets\n",
        "# Make sure to replace 'Train.csv' and 'Test.csv' with the actual file paths if they are not in the same directory as your notebook\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "\n",
        "# Check for missing values\n",
        "import numpy as np # Import numpy\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U0MIY2oQz6io",
        "outputId": "57cac2b2-2069-4165-cabf-c9158bd0c098"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages are installed!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-104e13e9f098>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Make sure to replace 'Train.csv' and 'Test.csv' with the actual file paths if they are not in the same directory as your notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train.csv'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Load from Google Drive (permanently available)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Step 4: Check and Impute Missing Values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train Model (Ultimate Auto-Fix Version)\n",
        "# Clean up column names (strip spaces and unify case)\n",
        "df_train.columns = df_train.columns.str.strip()\n",
        "df_test.columns = df_test.columns.str.strip()\n",
        "\n",
        "# Try fixing column name casing\n",
        "df_train.columns = df_train.columns.str.lower()\n",
        "df_test.columns = df_test.columns.str.lower()\n",
        "\n",
        "# Rename sample column too (for predictions)\n",
        "sample.columns = sample.columns.str.strip().str.lower()\n",
        "\n",
        "# Define correct label name\n",
        "label_col = 'gap' if 'gap' in df_train.columns else df_train.columns[-1]  # fallback to last column\n",
        "\n",
        "# Prepare train/test sets\n",
        "X_train = df_train.drop([label_col, 'id'], axis=1) if 'id' in df_train.columns else df_train.drop(label_col, axis=1)\n",
        "y_train = df_train[label_col]\n",
        "\n",
        "X_test = df_test.drop('id', axis=1) if 'id' in df_test.columns else df_test\n",
        "\n",
        "# Train model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Save\n",
        "sample[label_col] = predictions\n",
        "sample.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission file saved successfully.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N0X-FcFUH75f",
        "outputId": "76906488-3ea6-4652-8d84-62bf2d3afe12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training missing values:\n",
            " site           0\n",
            "PID            0\n",
            "lon            0\n",
            "lat            0\n",
            "pH             0\n",
            "alb            0\n",
            "bio1           0\n",
            "bio12          0\n",
            "bio15          0\n",
            "bio7           0\n",
            "bp             0\n",
            "cec20          0\n",
            "dows           0\n",
            "ecec20         5\n",
            "hp20           5\n",
            "ls             0\n",
            "lstd           0\n",
            "lstn           0\n",
            "mb1            0\n",
            "mb2            0\n",
            "mb3            0\n",
            "mb7            0\n",
            "mdem           0\n",
            "para           0\n",
            "parv           0\n",
            "ph20           0\n",
            "slope          0\n",
            "snd20          0\n",
            "soc20          0\n",
            "tim            0\n",
            "wp             0\n",
            "xhp20          5\n",
            "BulkDensity    4\n",
            "N              0\n",
            "P              0\n",
            "K              0\n",
            "Ca             0\n",
            "Mg             0\n",
            "S              0\n",
            "Fe             0\n",
            "Mn             0\n",
            "Zn             0\n",
            "Cu             0\n",
            "B              0\n",
            "dtype: int64\n",
            "\n",
            "Test missing values:\n",
            " site           0\n",
            "PID            0\n",
            "lon            0\n",
            "lat            0\n",
            "pH             0\n",
            "alb            0\n",
            "bio1           0\n",
            "bio12          0\n",
            "bio15          0\n",
            "bio7           0\n",
            "bp             0\n",
            "cec20          0\n",
            "dows           0\n",
            "ecec20         0\n",
            "hp20           0\n",
            "ls             0\n",
            "lstd           0\n",
            "lstn           0\n",
            "mb1            0\n",
            "mb2            0\n",
            "mb3            0\n",
            "mb7            0\n",
            "mdem           0\n",
            "para           0\n",
            "parv           0\n",
            "ph20           0\n",
            "slope          0\n",
            "snd20          0\n",
            "soc20          0\n",
            "tim            0\n",
            "wp             0\n",
            "xhp20          0\n",
            "BulkDensity    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-222bfa504f9c>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
            "<ipython-input-14-222bfa504f9c>:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_test[col].fillna(df_test[col].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'site_id_bIEHwl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-222bfa504f9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'site_id_bIEHwl'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Step 1: Mount Google Drive (Only once per session)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil  # For copying files\n",
        "import os     # For file path operations\n",
        "\n",
        "# Step 3: Define source and destination paths\n",
        "colab_data_path = \"/content/\"  # Where uploaded files are initially stored in Colab\n",
        "drive_data_path = \"/content/drive/MyDrive/soil_data/\"  # Your desired folder in Drive\n",
        "\n",
        "# Step 4: Copy files from Colab to Drive\n",
        "for filename in [\"Train.csv\", \"Test.csv\", \"SampleSubmission.csv\"]:\n",
        "    source_path = os.path.join(colab_data_path, filename)\n",
        "    destination_path = os.path.join(drive_data_path, filename)\n",
        "\n",
        "    # Check if the file exists in Colab before copying\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        print(f\"Copied {filename} to {drive_data_path}\")\n",
        "    else:\n",
        "        print(f\"Warning: {filename} not found in Colab.\")\n",
        "\n",
        "# Step 5: Load CSVs from Google Drive\n",
        "df_train = pd.read_csv(os.path.join(drive_data_path, 'Train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(drive_data_path, 'Test.csv'))\n",
        "sample = pd.read_csv(os.path.join(drive_data_path, 'SampleSubmission.csv'))\n",
        "\n",
        "# Step 6: Check for Missing Values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Step 7: Impute Missing Values (Mean Imputation)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Om3GcHc4ueEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# ✅ Step 3: Define paths\n",
        "local_files = ['Train.csv', 'Test.csv', 'SampleSubmission.csv']\n",
        "drive_folder = '/content/drive/MyDrive/soil_data/'\n",
        "\n",
        "# ✅ Step 4: Create folder in Drive if it doesn't exist\n",
        "if not os.path.exists(drive_folder):\n",
        "    os.makedirs(drive_folder)\n",
        "    print(f\"✅ Created folder: {drive_folder}\")\n",
        "\n",
        "# ✅ Step 5: Copy files from Colab to Drive\n",
        "for file in local_files:\n",
        "    local_path = os.path.join(\"/content/\", file)  # Explicitly define the local path\n",
        "    drive_path = os.path.join(drive_folder, file) # Explicitly define the drive path\n",
        "    if os.path.exists(local_path):\n",
        "        shutil.copy(local_path, drive_path)\n",
        "        print(f\"✅ {file} copied to Google Drive.\")\n",
        "    else:\n",
        "        print(f\"⚠️ {file} not found in Colab.\")\n",
        "\n",
        "# ✅ Step 6: Load files directly from Google Drive\n",
        "df_train = pd.read_csv(os.path.join(drive_folder, 'Train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(drive_folder, 'Test.csv'))\n",
        "sample = pd.read_csv(os.path.join(drive_folder, 'SampleSubmission.csv'))\n",
        "\n",
        "# ✅ Step 7: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# ✅ Step 8: Impute missing values for numeric columns\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OuQHzqnjviaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Upload the files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load the uploaded CSVs (update filenames as needed based on upload)\n",
        "df_train = pd.read_csv('Train (3).csv')\n",
        "df_test = pd.read_csv('Test (3).csv')\n",
        "sample = pd.read_csv('SampleSubmission (3).csv')  # Optional\n",
        "\n",
        "# Step 4: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Step 5: Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z9dI-Ua3RK0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load the files from your Drive folder\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')  # Optional\n",
        "\n",
        "# ✅ Step 4: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# ✅ Step 5: Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GDSAmhZY08S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load from permanent Google Drive path\n",
        "# Make sure you placed the files here:\n",
        "# My Drive > soil_data > Train.csv, Test.csv, SampleSubmission.csv\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# ✅ Step 4: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# ✅ Step 5: Impute missing numeric values\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rNgX0Bsj1mRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil, os\n",
        "\n",
        "# ✅ Step 3: Setup paths\n",
        "local_files = ['Train.csv', 'Test.csv', 'SampleSubmission.csv']\n",
        "drive_folder = '/content/drive/MyDrive/soil_data/'\n",
        "\n",
        "# ✅ Step 4: Create Drive folder if it doesn't exist\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# ✅ Step 5: Copy once and for all\n",
        "for file in local_files:\n",
        "    if os.path.exists(file):\n",
        "        shutil.copy(file, os.path.join(drive_folder, file))\n",
        "        print(f\"✅ {file} copied to Google Drive\")\n",
        "    else:\n",
        "        print(f\"⚠️ {file} not found in current directory\")\n",
        "\n",
        "# ✅ Step 6: Load from Drive forever — no more uploads needed!\n",
        "df_train = pd.read_csv(os.path.join(drive_folder, 'Train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(drive_folder, 'Test.csv'))\n",
        "sample = pd.read_csv(os.path.join(drive_folder, 'SampleSubmission.csv'))\n",
        "\n",
        "# ✅ Step 7: Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CBSKeToQzn_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')  # Make sure this file is uploaded\n",
        "df_test = pd.read_csv('Test.csv')    # Make sure this file is uploaded\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # If you're using it later\n",
        "\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cGgvy_Vtyyr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load the CSVs from Google Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train (3).csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test (3).csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission (3).csv')  # Optional\n",
        "\n",
        "# Step 4: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Step 5: Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lnrfyYqGkwsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load the CSVs from Google Drive\n",
        "# Make sure the path is correct and the files exist in your Google Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train (3).csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test (3).csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission (3).csv')  # Optional\n",
        "\n",
        "# Step 4: Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Step 5: Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3SGcYMDAlimN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7HRufK2MFsAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')  # Make sure this file is uploaded\n",
        "df_test = pd.read_csv('Test.csv')    # Make sure this file is uploaded\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # If you're using it later\n",
        "\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DGi84eRuFydR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "# Assuming the CSV files are in the same directory as the notebook\n",
        "# If they are in a different directory, replace 'Train.csv', 'Test.csv', and 'SampleSubmission.csv' with the correct file paths.\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # If you're using it later\n",
        "\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "43k8stOfF6iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Upload Your Files Every Time You Reopen Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt to upload manually\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Load the Uploaded CSVs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rMD1RQI8ISbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load files directly from Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# ✅ Step 4: Check and Impute Missing Values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# 🚀 Step 5: Train a Model (Simple Random Forest)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming 'label' is the target column in your training data\n",
        "X_train = df_train.drop(columns=['label'])\n",
        "y_train = df_train['label']\n",
        "\n",
        "# Create and train the model\n",
        "final_model = RandomForestRegressor(random_state=42)  # You can adjust hyperparameters\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Assuming 'X_test' is derived from 'df_test'\n",
        "# and you want to predict on the entire original test set\n",
        "predictions = final_model.predict(df_test.drop(columns=['ID']))\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission = pd.DataFrame({'ID': sample['ID'], 'Gap': predictions})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lGJV5mM87snc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Upload Your Files Every Time You Reopen Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt to upload manually\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Load the Uploaded CSVs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_train = pd.read_csv('Train (2).csv')  # Updated filename\n",
        "df_test = pd.read_csv('Test (2).csv')    # Updated filename\n",
        "sample = pd.read_csv('SampleSubmission (2).csv')  # Optional, updated filename\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SItm7oZcNu3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load CSV Files from Google Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# ✅ Step 4: Check for Missing Values and Impute\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute numeric missing values\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train a Simple Model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X_train = df_train.drop(['label'], axis=1)\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(['ID'], axis=1)  # Drop 'ID' column for prediction\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# ✅ Step 6: Save Submission File\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions  # Use 'Gap' column name from sample submission\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission saved at: /content/drive/MyDrive/soil_data/submission.csv\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gCfwhPSC89vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)  # Force remount to refresh credentials\n",
        "\n",
        "# ✅ Step 2: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load CSV Files from Google Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# ✅ Step 4: Check for Missing Values and Impute\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute numeric missing values\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train a Simple Model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X_train = df_train.drop(['label'], axis=1)\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(['ID'], axis=1)  # Drop 'ID' column for prediction\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# ✅ Step 6: Save Submission File\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions  # Use 'Gap' column name from sample submission\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission saved at: /content/drive/MyDrive/soil_data/submission.csv\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ri6TyG_69O92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "# Remount Google Drive only if it's not already mounted\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)  # Force remount to refresh credentials\n",
        "else:\n",
        "    print(\"Google Drive is already mounted!\")\n",
        "\n",
        "# ✅ Step 2: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Load CSV Files from Google Drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# ✅ Step 4: Check for Missing Values and Impute\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute numeric missing values\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train a Simple Model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X_train = df_train.drop(['label'], axis=1)\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(['ID'], axis=1)  # Drop 'ID' column for prediction\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# ✅ Step 6: Save Submission File\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions  # Use 'Gap' column name from sample submission\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission saved at: /content/drive/MyDrive/soil_data/submission.csv\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Pja3xk8G9bPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Axzddi7F-Hr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sUGmrKHr-Ky_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create target folder if it doesn't exist\n",
        "drive_path = '/content/drive/MyDrive/soil_data/'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Move files from Colab session storage to Google Drive\n",
        "for file in ['Train.csv', 'Test.csv', 'SampleSubmission.csv']:\n",
        "    if os.path.exists(file):  # Check if file exists in Colab session storage\n",
        "        shutil.move(file, drive_path + file)  # Move, not copy\n",
        "        print(f\"✅ {file} moved to Google Drive.\")\n",
        "    else:\n",
        "        print(f\"❌ {file} not found in Colab session.\")\n",
        "\n",
        "# Now load the data from Google Drive\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Proceed with your data processing and model training"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "b5jLn-w6_cOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "# Check if drive is already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Drive already mounted!\")\n",
        "\n",
        "# Create target folder if it doesn't exist\n",
        "drive_path = '/content/drive/MyDrive/soil_data/'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Move files from Colab session storage to Google Drive\n",
        "for file in ['Train.csv', 'Test.csv', 'SampleSubmission.csv']:\n",
        "    if os.path.exists(file):  # Check if file exists in Colab session storage\n",
        "        shutil.move(file, drive_path + file)  # Move, not copy\n",
        "        print(f\"✅ {file} moved to Google Drive.\")\n",
        "    else:\n",
        "        print(f\"❌ {file} not found in Colab session.\")\n",
        "\n",
        "# Now load the data from Google Drive\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Proceed with your data processing and model training"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J3kxxyhl_nMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "O1mI3I7w_t8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u8mDKUWj_6bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sgEDf0KCADrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "e1PEVPGrALat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "   from google.colab import drive\n",
        "   drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ik3oXey9BzWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!ls /content/drive/MyDrive/soil_data/"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1Dph1P18CS71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Load from Google Drive (permanently available)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Step 4: Check and Impute Missing Values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# Step 5: Train Model\n",
        "X_train = df_train.drop('label', axis=1)\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop('ID', axis=1) # Drop 'ID' from test data for prediction\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Step 6: Save Submission to Google Drive\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions  # Use 'Gap' for the target column in the submission file\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission file saved in Google Drive!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3Rk4EN2ODpip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Load from Google Drive (permanently available)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Step 4: Check and Impute Missing Values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# Step 5: Train Model\n",
        "# --- Updated to use the correct label column ---\n",
        "X_train = df_train.drop('target', axis=1)  # Assuming 'target' is the label column\n",
        "y_train = df_train['target']              # Assuming 'target' is the label column\n",
        "X_test = df_test.drop('ID', axis=1)       # Drop 'ID' from test data for prediction\n",
        "\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Step 6: Save Submission to Google Drive\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions  # Use 'Gap' for the target column in the submission file\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission file saved in Google Drive!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "udIGHlImEXXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train = df_train.drop('target', axis=1)\n",
        "   y_train = df_train['target']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zDiwRmGDEcXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "['ID', 'ph', 'N', 'P', 'K', 'organic_carbon', 'Gap']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mI5JA6UNFCEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 5: Train Model\n",
        "X_train = df_train.drop('Gap', axis=1)\n",
        "y_train = df_train['Gap']\n",
        "X_test = df_test.drop('ID', axis=1) if 'ID' in df_test.columns else df_test\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8DVjIKPtFEUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train = df_train.drop('Gap', axis=1)\n",
        "   y_train = df_train['Gap']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VGsCIOaKFH3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Load from Google Drive (permanently available)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Step 4: Check and Impute Missing Values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train Model (Updated and Corrected)\n",
        "# ✅ Step 1: Prepare features and label\n",
        "X_train = df_train.drop(['Gap', 'ID'], axis=1) if 'ID' in df_train.columns else df_train.drop('Gap', axis=1)\n",
        "y_train = df_train['Gap']\n",
        "\n",
        "# ✅ Step 2: Prepare test set\n",
        "X_test = df_test.drop('ID', axis=1) if 'ID' in df_test.columns else df_test\n",
        "\n",
        "# ✅ Step 3: Train model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ✅ Step 4: Predict\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Step 6: Save Submission to Google Drive\n",
        "# Make sure sample has same length as predictions\n",
        "submission = sample.copy()\n",
        "submission['Gap'] = predictions\n",
        "submission.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "\n",
        "print(\"✅ Submission saved successfully to your Drive.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l2BDtQHJGB4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Load from Google Drive (permanently available)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/soil_data/Train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/soil_data/Test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/soil_data/SampleSubmission.csv')\n",
        "\n",
        "# Step 4: Check and Impute Missing Values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)\n",
        "\n",
        "# ✅ Step 5: Train Model (Ultimate Auto-Fix Version)\n",
        "# Clean up column names (strip spaces and unify case)\n",
        "df_train.columns = df_train.columns.str.strip()\n",
        "df_test.columns = df_test.columns.str.strip()\n",
        "\n",
        "# Try fixing column name casing\n",
        "df_train.columns = df_train.columns.str.lower()\n",
        "df_test.columns = df_test.columns.str.lower()\n",
        "\n",
        "# Rename sample column too (for predictions)\n",
        "sample.columns = sample.columns.str.strip().str.lower()\n",
        "\n",
        "# Define correct label name\n",
        "label_col = 'gap' if 'gap' in df_train.columns else df_train.columns[-1]  # fallback to last column\n",
        "\n",
        "# Prepare train/test sets\n",
        "X_train = df_train.drop([label_col, 'id'], axis=1) if 'id' in df_train.columns else df_train.drop(label_col, axis=1)\n",
        "y_train = df_train[label_col]\n",
        "\n",
        "X_test = df_test.drop('id', axis=1) if 'id' in df_test.columns else df_test\n",
        "\n",
        "# Train model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Save\n",
        "sample[label_col] = predictions\n",
        "sample.to_csv('/content/drive/MyDrive/soil_data/submission.csv', index=False)\n",
        "print(\"✅ Submission file saved successfully.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wyi4e23ZGz2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Upload Your Files Every Time You Reopen Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt to upload manually\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Load the Uploaded CSVs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_train = pd.read_csv('Train (3).csv')  # Updated filename\n",
        "df_test = pd.read_csv('Test (3).csv')    # Updated filename\n",
        "sample = pd.read_csv('SampleSubmission (3).csv')  # Optional, updated filename\n",
        "\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OCrsGNz4O6Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qi5FkQPoLlBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kYlTclvADi50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Training missing values:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nTest missing values:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (only numeric columns)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aSzt2Mp3_atm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (only numeric columns)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cYzBjffl-VV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values for numeric columns\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nls1hkWH6vlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your training and test data here\n",
        "# Replace 'your_train_data.csv' and 'your_test_data.csv' with actual file paths\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J0mUQASc4Epf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your training and test data here\n",
        "# Replace 'your_train_data.csv' and 'your_test_data.csv' with actual file paths\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Wmq4wYmt2OEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')  # Make sure this file is uploaded\n",
        "df_test = pd.read_csv('Test.csv')    # Make sure this file is uploaded\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training data:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test data:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values for numeric columns using mean\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9-VkETE10aQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets first\n",
        "df_train = pd.read_csv('Train.csv')  # Assuming the file is in the current directory\n",
        "df_test = pd.read_csv('Test.csv')   # Assuming the file is in the current directory\n",
        "\n",
        "# Now proceed with the missing value check\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NDbVWXPNzWPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets first\n",
        "df_train = pd.read_csv('Train.csv')  # Assuming the file is in the current directory\n",
        "df_test = pd.read_csv('Test.csv')   # Assuming the file is in the current directory\n",
        "\n",
        "# Now proceed with the missing value check\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aHjiR3LBxpXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets first\n",
        "df_train = pd.read_csv('Train.csv')  # Assuming the file is in the current directory\n",
        "df_test = pd.read_csv('Test.csv')   # Assuming the file is in the current directory\n",
        "\n",
        "# Now proceed with the missing value check\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "V_YBQrXdt4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "import numpy as np # Import numpy\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yXG6RPbGCR60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()\n"
      ],
      "metadata": {
        "id": "qT9lbm0Nn3u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')\n",
        "# test = pd.read_csv('Test.csv')\n",
        "# sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "k53c_cYpobDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Updated path\n",
        "test = pd.read_csv('Test.csv')    # Updated path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yujdDMHEovKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# test = pd.read_csv('Test.csv')    # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "\n",
        "train = pd.read_csv('Train.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "test = pd.read_csv('Test.csv')    # Path without 'data/' prefix (use if files are in the current directory)\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vrCytO_2puJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pXL5RlUapk31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RYCgP5r6pY92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OW8T0F8wpPCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cTFbYYk8pHIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ElEP_vS9o8NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tat8DoVHo0DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# Assuming the files are in the current directory, remove the 'data/' prefix\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "w7JT8NSSor0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# test = pd.read_csv('Test.csv')    # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "\n",
        "train = pd.read_csv('Train.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "test = pd.read_csv('Test.csv')    # Path without 'data/' prefix (use if files are in the current directory)\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9G6G8M1voj4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oGVH22eeoYyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GdcmkSwaoJ8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Updated path\n",
        "# test = pd.read_csv('Test.csv')    # Updated path\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Updated path\n",
        "\n",
        "# The actual files seems to be in the current directory, so remove 'data/' prefix:\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nujLAYI1n-9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# test = pd.read_csv('Test.csv')    # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "\n",
        "# The files 'Train.csv', 'Test.csv', and 'SampleSubmission.csv'\n",
        "# should be in the same directory as this notebook,\n",
        "# OR you should specify the correct path to them.\n",
        "\n",
        "# These files have already been loaded earlier, so we can reuse those variables:\n",
        "# df_train = train  # Assuming 'train' is the DataFrame you loaded earlier\n",
        "# df_test = test    # Assuming 'test' is the DataFrame you loaded earlier\n",
        "# df_submission = sample  # Assuming 'sample' is the DataFrame you loaded earlier\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IE_X_Cjhny8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# test = pd.read_csv('Test.csv')    # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "\n",
        "train = pd.read_csv('Train.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "test = pd.read_csv('Test.csv')    # Path without 'data/' prefix (use if files are in the current directory)\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "drd-BcxTnne7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "# train = pd.read_csv('Train.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# test = pd.read_csv('Test.csv')    # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "# sample = pd.read_csv('SampleSubmission.csv')  # Path with 'data/' prefix (use if files are in 'data' folder)\n",
        "\n",
        "train = pd.read_csv('Train.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "test = pd.read_csv('Test.csv')    # Path without 'data/' prefix (use if files are in the current directory)\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Path without 'data/' prefix (use if files are in the current directory)\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EbhPSkPPneRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JXFqFWJfnRbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "train = pd.read_csv('Train.csv')  # Corrected path\n",
        "test = pd.read_csv('Test.csv')    # Corrected path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Corrected path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "twaMEky-m-k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' prefix\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' prefix\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' prefix\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EBrQpwUdmUz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ijnjBxwQmA0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "COMgJEAGl1zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' prefix\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' prefix\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' prefix\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "H6T9q5Pflo7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8YdzOhyLkxAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load the datasets\n",
        "# If the files are in a different directory, specify the full path.\n",
        "# For example, if the files are in a 'data' folder within the current directory:\n",
        "train = pd.read_csv('Train.csv')  # Removed 'data/' from the path\n",
        "test = pd.read_csv('Test.csv')    # Removed 'data/' from the path\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Removed 'data/' from the path\n",
        "\n",
        "# View basic info\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train columns:\", train.columns.tolist())\n",
        "train.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GF1mwd4BkoKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PkqWD4l-kV2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "    print(df_train.isnull().sum())\n",
        "    print(df_test.isnull().sum())\n",
        "\n",
        "    # Impute missing values (example using mean imputation)\n",
        "    df_train.fillna(df_train.mean(), inplace=True)\n",
        "    df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UAO_MWSO9pP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Upload Your Files Every Time You Reopen Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt to upload manually\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Load the Uploaded CSVs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_train = pd.read_csv('Train (3).csv')  # Updated filename\n",
        "df_test = pd.read_csv('Test (3).csv')    # Updated filename\n",
        "sample = pd.read_csv('SampleSubmission (3).csv')  # Optional, updated filename\n",
        "\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mnOmL5y-QWUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Fx2KWvwBEJHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S_F3o_sMDZdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Nf6ZwNYECjqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "njHDh0kHB1D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (only numeric columns)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I8wJFO_wBTll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CLHsb5hTAUWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (only numeric columns)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wce_hJK4-1gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional, only if you're using it\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dckux3vH8zHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # Optional\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training set:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test set:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (only numeric columns)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Jpgqbg359joW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')\n",
        "df_test = pd.read_csv('Test.csv')\n",
        "\n",
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Qj6gEbp35lAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bCQLOGQe4Xj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VcSOgm0J4elo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PZHw57S-2bdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv('Train.csv')         # Make sure this file is uploaded\n",
        "df_test = pd.read_csv('Test.csv')           # Make sure this file is uploaded\n",
        "sample = pd.read_csv('SampleSubmission.csv')  # If you're using it later\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in training data:\\n\", df_train.isnull().sum())\n",
        "print(\"\\nMissing values in test data:\\n\", df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5jue8Ez91W7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "numeric_cols = df_train.select_dtypes(include=np.number).columns\n",
        "df_train[numeric_cols] = df_train[numeric_cols].fillna(df_train[numeric_cols].mean())\n",
        "\n",
        "numeric_cols = df_test.select_dtypes(include=np.number).columns\n",
        "df_test[numeric_cols] = df_test[numeric_cols].fillna(df_test[numeric_cols].mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GnuiXBBy1i_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4KkwjkOVzuLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets first\n",
        "df_train = pd.read_csv('Train.csv')  # Assuming the file is in the current directory\n",
        "df_test = pd.read_csv('Test.csv')   # Assuming the file is in the current directory\n",
        "\n",
        "# Now proceed with the missing value check\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "v1ncrzsAyKZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "# Only select numeric columns for mean imputation\n",
        "df_train[df_train.select_dtypes(include='number').columns] = df_train.select_dtypes(include='number').fillna(df_train.select_dtypes(include='number').mean())\n",
        "df_test[df_test.select_dtypes(include='number').columns] = df_test.select_dtypes(include='number').fillna(df_test.select_dtypes(include='number').mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uSVYU57iwdof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jagJnn5gvDCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "# Only select numeric columns for mean imputation\n",
        "df_train[df_train.select_dtypes(include='number').columns] = df_train.select_dtypes(include='number').fillna(df_train.select_dtypes(include='number').mean())\n",
        "df_test[df_test.select_dtypes(include='number').columns] = df_test.select_dtypes(include='number').fillna(df_test.select_dtypes(include='number').mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Rc3TL8Ogv2oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zUrzi5ukumxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_XLMYfvhuMSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "numeric_cols_train = df_train.select_dtypes(include=np.number).columns\n",
        "df_train[numeric_cols_train] = df_train[numeric_cols_train].fillna(df_train[numeric_cols_train].mean())\n",
        "\n",
        "numeric_cols_test = df_test.select_dtypes(include=np.number).columns\n",
        "df_test[numeric_cols_test] = df_test[numeric_cols_test].fillna(df_test[numeric_cols_test].mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5VhgMi-ouxVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ppO6_2PCtaZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "numeric_cols_train = df_train.select_dtypes(include=np.number).columns\n",
        "df_train[numeric_cols_train] = df_train[numeric_cols_train].fillna(df_train[numeric_cols_train].mean())\n",
        "\n",
        "numeric_cols_test = df_test.select_dtypes(include=np.number).columns\n",
        "df_test[numeric_cols_test] = df_test[numeric_cols_test].fillna(df_test[numeric_cols_test].mean())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Bqqke0UntlSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qGhbzQXKtTWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "35DLR3x1tJtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation)\n",
        "df_train.fillna(df_train.mean(), inplace=True)\n",
        "df_test.fillna(df_test.mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hx3nnlKr9uwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check for missing values\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UwRMWDMC96in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the datasets first\n",
        "df_train = pd.read_csv('Train.csv')  # Assuming the file is in the current directory\n",
        "df_test = pd.read_csv('Test.csv')   # Assuming the file is in the current directory\n",
        "\n",
        "# Now proceed with the missing value check\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YEmrSMpHjBXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "# Check for missing values\n",
        "import numpy as np # Import numpy\n",
        "print(df_train.isnull().sum())\n",
        "print(df_test.isnull().sum())\n",
        "\n",
        "# Impute missing values (example using mean imputation for numeric columns only)\n",
        "for col in df_train.select_dtypes(include=np.number).columns:\n",
        "    df_train[col].fillna(df_train[col].mean(), inplace=True)\n",
        "for col in df_test.select_dtypes(include=np.number).columns:\n",
        "    df_test[col].fillna(df_test[col].mean(), inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LD_D7up8-Adm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Task\n",
        "Tell me about this dataset.\n",
        "\n",
        "Here is all the data you need:\n",
        "\"Train.csv\"\n",
        "\"Test.csv\"\n",
        "\"SampleSubmission.csv\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q7wtN8zpHCj"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the provided CSV files into pandas DataFrames.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "4f2GlSZlpHSZ"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Load the three CSV files into pandas DataFrames and display their first five rows.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "5nVHC8_xpHiK"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df_train = pd.read_csv('Train.csv')\n",
        "    df_test = pd.read_csv('Test.csv')\n",
        "    df_submission = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "    display(df_train.head())\n",
        "    display(df_test.head())\n",
        "    display(df_submission.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or more CSV files not found.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error: Unable to parse one or more CSV files.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Td323rCbpHx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the \"Train.csv\" DataFrame.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "BYoIsrvFpLo0"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Explore the 'Train.csv' DataFrame by examining its shape, data types, summary statistics, missing values, unique values in categorical columns, and distributions of numerical features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YfMWLMhPpMot"
      }
    },
    {
      "source": [
        "# Examine the shape of the DataFrame.\n",
        "print(\"Shape of df_train:\", df_train.shape)\n",
        "\n",
        "# Determine the data types of each column.\n",
        "print(\"\\nData types of df_train columns:\\n\", df_train.dtypes)\n",
        "\n",
        "# Generate summary statistics for numerical columns.\n",
        "print(\"\\nSummary statistics of df_train:\\n\", df_train.describe())\n",
        "\n",
        "# Check for missing values.\n",
        "print(\"\\nMissing values in df_train:\\n\", df_train.isnull().sum())\n",
        "\n",
        "# Identify unique values in categorical columns.\n",
        "categorical_cols = df_train.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nUnique values in column '{col}':\\n{df_train[col].unique()}\")\n",
        "    print(f\"Number of unique values in column '{col}': {df_train[col].nunique()}\")\n",
        "\n",
        "# Examine distributions of numerical features (using histograms).\n",
        "import matplotlib.pyplot as plt\n",
        "numerical_cols = df_train.select_dtypes(include=['number']).columns\n",
        "num_plots = len(numerical_cols)\n",
        "num_cols = 3\n",
        "num_rows = (num_plots + num_cols - 1) // num_cols\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    if i < num_plots:\n",
        "      df_train[col].hist(ax=axes[i], color='skyblue', edgecolor='black')\n",
        "      axes[i].set_title(f'Distribution of {col}')\n",
        "      axes[i].set_xlabel(col)\n",
        "      axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VOKXGe-9pM4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the \"Test.csv\" DataFrame and compare it to the \"Train.csv\" DataFrame.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "hTVzKr3PpT5U"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "I need to explore the `df_test` DataFrame and compare it to `df_train` by examining their shapes, data types, summary statistics, missing values, and unique values in categorical columns. I'll then summarize the comparisons, highlighting any significant differences.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "AxeqCuTopUwG"
      }
    },
    {
      "source": [
        "# Examine the shape of the DataFrame.\n",
        "print(\"Shape of df_test:\", df_test.shape)\n",
        "print(\"Shape of df_train:\", df_train.shape)\n",
        "\n",
        "# Determine the data types of each column.\n",
        "print(\"\\nData types of df_test columns:\\n\", df_test.dtypes)\n",
        "print(\"\\nData types of df_train columns:\\n\", df_train.dtypes)\n",
        "\n",
        "# Generate summary statistics for numerical columns.\n",
        "print(\"\\nSummary statistics of df_test:\\n\", df_test.describe())\n",
        "print(\"\\nSummary statistics of df_train:\\n\", df_train.describe())\n",
        "\n",
        "# Check for missing values.\n",
        "print(\"\\nMissing values in df_test:\\n\", df_test.isnull().sum())\n",
        "print(\"\\nMissing values in df_train:\\n\", df_train.isnull().sum())\n",
        "\n",
        "# Identify unique values in categorical columns.\n",
        "categorical_cols_test = df_test.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols_test:\n",
        "    print(f\"\\nUnique values in df_test column '{col}':\\n{df_test[col].unique()}\")\n",
        "    print(f\"Number of unique values in df_test column '{col}': {df_test[col].nunique()}\")\n",
        "\n",
        "categorical_cols_train = df_train.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols_train:\n",
        "    print(f\"\\nUnique values in df_train column '{col}':\\n{df_train[col].unique()}\")\n",
        "    print(f\"Number of unique values in df_train column '{col}': {df_train[col].nunique()}\")\n",
        "\n",
        "print(\"\\nSummary of Comparison:\")\n",
        "# Add more detailed comparison logic here as needed."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MHmRmancpU_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "I need to complete the analysis by comparing the unique values in categorical columns between the two dataframes and summarize the findings.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "BnhF_v2upXbN"
      }
    },
    {
      "source": [
        "# Identify unique values in categorical columns.  We already have this, but let's make it clearer.\n",
        "\n",
        "print(\"\\nComparison of Unique Values in Categorical Columns:\")\n",
        "categorical_cols_test = df_test.select_dtypes(include=['object']).columns\n",
        "categorical_cols_train = df_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_cols_test:\n",
        "    if col in categorical_cols_train:\n",
        "        unique_test = set(df_test[col].unique())\n",
        "        unique_train = set(df_train[col].unique())\n",
        "\n",
        "        print(f\"\\nColumn '{col}':\")\n",
        "        print(f\"  Unique values in df_test: {unique_test}\")\n",
        "        print(f\"  Unique values in df_train: {unique_train}\")\n",
        "        print(f\"  Values in df_train but not in df_test: {unique_train - unique_test}\")\n",
        "        print(f\"  Values in df_test but not in df_train: {unique_test - unique_train}\")\n",
        "    else:\n",
        "        print(f\"\\nColumn '{col}' is present in df_test but not in df_train.\")\n",
        "\n",
        "for col in categorical_cols_train:\n",
        "    if col not in categorical_cols_test:\n",
        "        print(f\"\\nColumn '{col}' is present in df_train but not in df_test.\")\n",
        "\n",
        "print(\"\\nSummary of Comparison:\")\n",
        "print(\"1. Shape: df_test has fewer rows (2418 vs 7744) and fewer columns (33 vs 44) than df_train.\")\n",
        "print(\"2. Data Types: Most columns have consistent data types.  However, df_train has additional columns ('N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B') that are not present in df_test.\")\n",
        "print(\"3. Summary Statistics:  Numerical features show some differences in their distributions (means, standard deviations, and ranges) between the two datasets.\")\n",
        "print(\"4. Missing Values:  The patterns of missing values might differ between datasets.  Further investigation is needed.\")\n",
        "print(\"5. Unique Values in Categorical Columns:  The analysis above shows the presence and absence of unique values in categorical columns. This difference in categories could be important for modeling.\")\n",
        "print(\"6. Overall: The key differences are the number of rows/columns, presence of 'N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', and 'B' columns in df_train, and variations in distributions and unique categorical values.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "P9wnGP-LpXq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the \"SampleSubmission.csv\" DataFrame and understand its format.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YDzsrUeapcQd"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "I need to explore the `df_submission` DataFrame to understand its format, including its shape, data types, missing values, and unique IDs. I will also compare the number of unique IDs to the number of rows in `df_test` to check for consistency.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "WRekU6jCpdEw"
      }
    },
    {
      "source": [
        "# Examine the shape of the DataFrame.\n",
        "print(\"Shape of df_submission:\", df_submission.shape)\n",
        "\n",
        "# Determine the data types of each column.\n",
        "print(\"\\nData types of df_submission columns:\\n\", df_submission.dtypes)\n",
        "\n",
        "# Check for missing values.\n",
        "print(\"\\nMissing values in df_submission:\\n\", df_submission.isnull().sum())\n",
        "\n",
        "# Examine the unique values in the 'ID' column.\n",
        "unique_ids = df_submission['ID'].nunique()\n",
        "print(f\"\\nNumber of unique IDs in df_submission: {unique_ids}\")\n",
        "print(f\"Number of rows in df_test: {df_test.shape[0]}\")\n",
        "\n",
        "# Compare the number of unique IDs to the number of rows in df_test.\n",
        "if unique_ids == df_test.shape[0]:\n",
        "    print(\"\\nThe number of unique IDs in df_submission matches the number of rows in df_test.\")\n",
        "else:\n",
        "    print(\"\\nThe number of unique IDs in df_submission does not match the number of rows in df_test.\")\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(\"The 'SampleSubmission.csv' file should contain predictions for each ID in the test set.\")\n",
        "print(f\"The target variable is 'Gap' and its data type is {df_submission['Gap'].dtype}.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fXTsq7P7pdUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the relationship between features in the \"Train.csv\" data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWKWu1fYpg0L"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Analyze the relationship between features in the \"Train.csv\" data by calculating the correlation matrix, exploring relationships between numerical features and the target variable, and investigating relationships between categorical features and other variables.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "n515m_Japhot"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the correlation matrix for numerical features\n",
        "numerical_features = df_train.select_dtypes(include=['number']).columns\n",
        "correlation_matrix = df_train[numerical_features].corr()\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "# Explore relationships between numerical features and the target variable (if one exists)\n",
        "if 'Gap' in df_train.columns:\n",
        "    target_variable = 'Gap'\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, col in enumerate(numerical_features):\n",
        "        plt.subplot(4, 4, i + 1)  # Adjust the grid as needed\n",
        "        plt.scatter(df_train[col], df_train[target_variable], alpha=0.5)\n",
        "        plt.title(f'{col} vs. {target_variable}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel(target_variable)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate correlation coefficients\n",
        "    correlations_with_target = df_train[numerical_features].corrwith(df_train[target_variable])\n",
        "    print(correlations_with_target)\n",
        "else:\n",
        "    print(\"No target variable 'Gap' found in the dataframe\")\n",
        "\n",
        "# Investigate relationships between categorical features and other variables\n",
        "categorical_features = df_train.select_dtypes(include=['object']).columns\n",
        "for col in categorical_features:\n",
        "    print(f\"\\nAnalyzing categorical feature: {col}\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for num_col in numerical_features:\n",
        "        sns.boxplot(x=col, y=num_col, data=df_train)\n",
        "        plt.title(f'Boxplot of {num_col} by {col}')\n",
        "        plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fw5HhEtRph4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Summary:\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "There is no explicit question in the provided data analysis task. However, the tasks implicitly ask for an understanding of the datasets and relationships between features.  Based on the analysis:\n",
        "\n",
        "*   **What are the key differences between the training and test datasets?**  The training dataset (`df_train`) has more rows and columns than the test dataset (`df_test`).  Critically, `df_train` includes 11 nutrient level columns ('N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B') not present in `df_test`.  The distributions of numerical features and unique categorical values also differ between the two datasets.\n",
        "*   **Is the `SampleSubmission.csv` file correctly formatted for the provided test data?** No. The number of unique IDs in the sample submission file does not match the number of rows in the test dataset, suggesting an incompatibility.\n",
        "*   **What is the target variable?**  A 'Gap' column was expected as a target variable, but it wasn't found in the `df_train` data. This significantly impacts the intended analysis.\n",
        "\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Discrepancies:** `df_train` (7744 rows, 44 columns) and `df_test` (2418 rows, 33 columns) have significant structural differences, notably the absence of 11 nutrient columns in `df_test`.\n",
        "*   **Missing Target Variable:** The expected target variable, 'Gap', is not present in `df_train`, hindering analysis of feature relationships with the target.\n",
        "*   **Inconsistent Sample Submission:** The `df_submission` file contains 26,598 unique IDs, which doesn't correspond to the 2,418 rows in `df_test`.  The submission file is likely incorrect or not designed for this test set.\n",
        "\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "*   **Investigate Missing Target Variable:** Determine the correct target variable or source for the 'Gap' data. Without it, predictive modeling is impossible. Explore alternative target variables in the training or supplementary datasets (if available)\n",
        "*   **Reconcile Dataset and Submission Issues:**  Clarify the intended use of `df_test` and `df_submission`. Obtain the correct test data or submission file template if necessary.  Consider whether the missing 'N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', and 'B' features in the test set should be imputed or addressed in another way before prediction.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "2T8Sip1m5Ony"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimated requirements (kg/ha) per 4t maize yield\n",
        "nutrient_requirements = {\n",
        "    'N': 120,  # Nitrogen\n",
        "    'P': 60,   # Phosphorus\n",
        "    'K': 90,   # Potassium\n",
        "    'Ca': 20,  # Calcium\n",
        "    'Mg': 15,  # Magnesium\n",
        "    'S': 10,   # Sulfur\n",
        "    'Fe': 2.5, # Iron\n",
        "    'Mn': 2.0, # Manganese\n",
        "    'Zn': 1.5, # Zinc\n",
        "    'Cu': 1.0, # Copper\n",
        "    'B': 0.5   # Boron\n",
        "}\n"
      ],
      "metadata": {
        "id": "iKmr_Sa7UltM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training set in long format\n",
        "import pandas as pd\n",
        "\n",
        "nutrient_cols = list(nutrient_requirements.keys())\n",
        "\n",
        "long_train = pd.melt(\n",
        "    train,\n",
        "    id_vars=[col for col in train.columns if col not in nutrient_cols],\n",
        "    value_vars=nutrient_cols,\n",
        "    var_name='Nutrient',\n",
        "    value_name='Available'\n",
        ")\n",
        "\n",
        "# Calculate Gap = Required - Available\n",
        "long_train['Gap'] = long_train.apply(\n",
        "    lambda row: nutrient_requirements[row['Nutrient']] - row['Available'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Create a unique PID like in SampleSubmission.csv\n",
        "long_train['PID'] = long_train['PID'].astype(str) + \"_\" + long_train['Nutrient']\n"
      ],
      "metadata": {
        "id": "47vBbkkOUsn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create the training set in long format\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training data if it's not already loaded\n",
        "try:\n",
        "    train\n",
        "except NameError:\n",
        "    train = pd.read_csv('Train.csv') # Load the 'Train.csv' file\n",
        "\n",
        "nutrient_cols = list(nutrient_requirements.keys())\n",
        "\n",
        "long_train = pd.melt(\n",
        "    train,\n",
        "    id_vars=[col for col in train.columns if col not in nutrient_cols],\n",
        "    value_vars=nutrient_cols,\n",
        "    var_name='Nutrient',\n",
        "    value_name='Available'\n",
        ")\n",
        "\n",
        "# Calculate Gap = Required - Available\n",
        "long_train['Gap'] = long_train.apply(\n",
        "    lambda row: nutrient_requirements[row['Nutrient']] - row['Available'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Create a unique PID like in SampleSubmission.csv\n",
        "long_train['PID'] = long_train['PID'].astype(str) + \"_\" + long_train['Nutrient']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xpmeqSoZU9ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train for Nitrogen (N)\n",
        "n_df = long_train[long_train['Nutrient'] == 'N'].copy()\n",
        "\n",
        "features = [col for col in n_df.columns if col not in ['PID', 'Gap', 'Nutrient', 'Available']]\n",
        "X = n_df[features]\n",
        "y = n_df['Gap']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training complete ✅\")\n"
      ],
      "metadata": {
        "id": "4ulM41FhVBIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train for Nitrogen (N)\n",
        "n_df = long_train[long_train['Nutrient'] == 'N'].copy()\n",
        "\n",
        "# Exclude 'PID' from the features\n",
        "features = [col for col in n_df.columns if col not in ['PID', 'Gap', 'Nutrient', 'Available']]\n",
        "X = n_df[features]\n",
        "y = n_df['Gap']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training complete ✅\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "C-B_ijymVQOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Train for Nitrogen (N)\n",
        "n_df = long_train[long_train['Nutrient'] == 'N'].copy()\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Train for Nitrogen (N)\n",
        "n_df = long_train[long_train['Nutrient'] == 'N'].copy()\n",
        "\n",
        "# Exclude 'PID' from the features and encode 'site'\n",
        "features = [col for col in n_df.columns if col not in ['PID', 'Gap', 'Nutrient']]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "olDuyXLWVW4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test set\n",
        "test['N_PRED'] = rf.predict(test[features])\n"
      ],
      "metadata": {
        "id": "pfjYThP-VwIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Prepare test set\n",
        "import pandas as pd # Import pandas to load data\n",
        "\n",
        "# Load the test data if it's not already loaded\n",
        "try:\n",
        "    test\n",
        "except NameError:\n",
        "    test = pd.read_csv('Test.csv') # Load the 'Test.csv' file\n",
        "\n",
        "# Ensure features used during training are present in the test set\n",
        "# Get the columns present in the test dataset\n",
        "test_columns = test.columns.tolist()\n",
        "\n",
        "# Check if any features used during training are not in the test dataset\n",
        "missing_features = [f for f in features if f not in test_columns]\n",
        "\n",
        "if missing_features:\n",
        "    # Handle missing features (e.g., impute them with 0 or mean)\n",
        "    for feature in missing_features:\n",
        "        print(f\"Feature '{feature}' missing in test set. Imputing with 0.\")\n",
        "        test[feature] = 0  # Replace 0 with your imputation strategy\n",
        "\n",
        "\n",
        "test['N_PRED'] = rf.predict(test[features])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lq4_W4UWWEwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Prepare test set\n",
        "import pandas as pd # Import pandas to load data\n",
        "\n",
        "# Load the test data if it's not already loaded\n",
        "try:\n",
        "    test\n",
        "except NameError:\n",
        "    test = pd.read_csv('Test.csv') # Load the 'Test.csv' file\n",
        "\n",
        "# Ensure features used during training are present in the test set\n",
        "# Get the columns present in the test dataset\n",
        "test_columns = test.columns.tolist()\n",
        "\n",
        "# Check if any features used during training are not in the test dataset\n",
        "missing_features = [f for f in features if f not in test_columns]\n",
        "\n",
        "if missing_features:\n",
        "    # Handle missing features (e.g., impute them with 0 or mean)\n",
        "    for feature in missing_features:\n",
        "        print(f\"Feature '{feature}' missing in test set. Imputing with 0.\")\n",
        "        test[feature] = 0  # Replace 0 with your imputation strategy\n",
        "\n",
        "# Retrain the model before predicting\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train for Nitrogen (N)\n",
        "n_df = long_train[long_train['Nutrient'] == 'N'].copy()\n",
        "\n",
        "# Exclude 'PID' from the features and encode 'site'\n",
        "features = [col for col in n_df.columns if col not in ['PID', 'Gap', 'Nutrient']]  # Assuming 'Available' is not needed\n",
        "X = n_df[features]\n",
        "y = n_df['Gap']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)  # Fit the model again here\n",
        "\n",
        "\n",
        "test['N_PRED'] = rf.predict(test[features])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nXQuk5orWQCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Prepare test set\n",
        "import pandas as pd # Import pandas to load data\n",
        "\n",
        "# Load the test data if it's not already loaded\n",
        "try:\n",
        "    test\n",
        "except NameError:\n",
        "    test = pd.read_csv('Test.csv') # Load the 'Test.csv' file\n",
        "\n",
        "# Ensure features used during training are present in the test set\n",
        "# Get the columns present in the test dataset\n",
        "test_columns = test.columns.tolist()\n",
        "# Prepare test set\n",
        "import pandas as pd # Import pandas to load data\n",
        "\n",
        "# Load the test data if it's not already loaded\n",
        "try:\n",
        "    test\n",
        "except NameError:\n",
        "    test = pd.read_csv('Test.csv') # Load the 'Test.csv' file\n",
        "\n",
        "# Ensure features used during training are present in the test set\n",
        "# Get the columns present in the test dataset\n",
        "test_columns = test.columns.tolist()\n",
        "\n",
        "# Check if any features used during training are not in the test dataset\n",
        "missing_features = [f for f in features if f not in test_columns]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DUpOzHL-WVQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "submission['PID'] = test['PID'].astype(str) + '_N'  # Adjust later for all nutrients\n",
        "submission['Gap'] = test['N_PRED']\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Vo_XGPpEWrEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Prepare test set and make predictions\n",
        "import pandas as pd # Import pandas to load data\n",
        "\n",
        "# Load the test data if it's not already loaded\n",
        "try:\n",
        "    test\n",
        "except NameError:\n",
        "    test = pd.read_csv('Test.csv') # Load the 'Test.csv' file\n",
        "\n",
        "# Ensure features used during training are present in the test set\n",
        "#"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "M4RYX5HYWyfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# 1. Nutrient requirements (based on 4 tons/ha maize yield)\n",
        "nutrient_requirements = {\n",
        "    'N': 120, 'P': 60, 'K': 90, 'Ca': 20, 'Mg': 15,\n",
        "    'S': 10, 'Fe': 2.5, 'Mn': 2.0, 'Zn': 1.5, 'Cu': 1.0, 'B': 0.5\n",
        "}\n",
        "\n",
        "# 2. Reshape train data to long format\n",
        "nutrient_cols = list(nutrient_requirements.keys())\n",
        "\n",
        "long_train = pd.melt(\n",
        "    train,\n",
        "    id_vars=[col for col in train.columns if col not in nutrient_cols],\n",
        "    value_vars=nutrient_cols,\n",
        "    var_name='Nutrient',\n",
        "    value_name='Available'\n",
        ")\n",
        "\n",
        "# 3. Calculate Gap = Requirement - Available\n",
        "long_train['Gap'] = long_train.apply(\n",
        "    lambda row: nutrient_requirements[row['Nutrient']] - row['Available'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 4. Loop through each nutrient, train a model and predict\n",
        "all_preds = []\n",
        "\n",
        "for nutrient in nutrient_cols:\n",
        "    print(f\"Training for nutrient: {nutrient}\")\n",
        "\n",
        "    # Subset data for the nutrient\n",
        "    df = long_train[long_train['Nutrient'] == nutrient].copy()\n",
        "\n",
        "    # Drop unneeded columns\n",
        "    drop_cols = ['PID', 'Available', 'Nutrient', 'Gap']\n",
        "    features = [col for col in df.columns if col not in drop_cols]\n",
        "\n",
        "    # Define train features and target\n",
        "    X_train = df[features]\n",
        "    y_train = df['Gap']\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prepare test data (keep only matching columns)\n",
        "    X_test = test[features].copy()\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Build prediction DataFrame\n",
        "    pred_df = pd.DataFrame({\n",
        "        'PID': test['PID'].astype(str) + f'_{nutrient}',\n",
        "        'Gap': preds\n",
        "    })\n",
        "\n",
        "    all_preds.append(pred_df)\n",
        "\n",
        "# 5. Combine all predictions\n",
        "submission = pd.concat(all_preds)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"✅ Submission file saved successfully!\")\n"
      ],
      "metadata": {
        "id": "_XBdx6BIXsob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Nutrient requirements (based on 4 tons/ha maize yield)\n",
        "nutrient_requirements = {\n",
        "    'N': 120, 'P': 60, 'K': 90, 'Ca': 20, 'Mg': 15,\n",
        "    'S': 10, 'Fe': 2.5, 'Mn': 2.0, 'Zn': 1.5, 'Cu': 1.0, 'B': 0.5\n",
        "}\n",
        "\n",
        "# 2. Reshape train data to long format\n",
        "nutrient_cols = list(nutrient_requirements.keys())\n",
        "\n",
        "long_train = pd.melt(\n",
        "    train,\n",
        "    id_vars=[col for col in train.columns if col not in nutrient_cols],\n",
        "    value_vars=nutrient_cols,\n",
        "    var_name='Nutrient',\n",
        "    value_name='Available'\n",
        ")\n",
        "\n",
        "# 3. Calculate Gap = Requirement - Available\n",
        "long_train['Gap'] = long_train.apply(\n",
        "    lambda row: nutrient_requirements[row['Nutrient']] - row['Available'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 4. Loop through each nutrient, train a model and predict\n",
        "all_preds = []\n",
        "\n",
        "for nutrient in nutrient_cols:\n",
        "    print(f\"Training for nutrient: {nutrient}\")\n",
        "\n",
        "    # Subset data for the nutrient\n",
        "    df = long_train[long_train['Nutrient'] == nutrient].copy()\n",
        "\n",
        "    # Drop unneeded columns\n",
        "    drop_cols = ['PID', 'Available', 'Nutrient', 'Gap']\n",
        "    features = [col for col in df.columns if col not in drop_cols]\n",
        "\n",
        "    # Define train features and target\n",
        "    X_train = df[features]\n",
        "    y_train = df['Gap']\n",
        "\n",
        "    # ---->  Encode categorical features (like 'site') <----\n",
        "    for col in X_train.select_dtypes(include=['object']).columns:\n",
        "        le = LabelEncoder()\n",
        "        X_train[col] = le.fit_transform(X_train[col])\n",
        "        # Apply the same encoding to the test set\n",
        "        test[col] = le.transform(test[col])\n",
        "\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prepare test data (keep only matching columns)\n",
        "    X_test = test[features].copy()\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Build prediction DataFrame\n",
        "    pred_df = pd.DataFrame({\n",
        "        'PID': test['PID'].astype(str) + f'_{nutrient}',\n",
        "        'Gap': preds\n",
        "    })\n",
        "\n",
        "    all_preds.append(pred_df)\n",
        "\n",
        "# 5. Combine all predictions\n",
        "submission = pd.concat(all_preds)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"✅ Submission file saved successfully!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0A1bOcdpYEhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "le = LabelEncoder()\n",
        "X_train[col] = le.fit_transform(X_train[col])  # Fit on training data\n",
        "test[col] = le.transform(test[col])         # Transform test data"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cds3VZpvYMTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "for nutrient in nutrient_cols:\n",
        "    # ... (subset data, drop columns, etc.)\n",
        "\n",
        "    # Encode categorical features (like 'site')\n",
        "    for col in X_train.select_dtypes(include=['object']).columns:\n",
        "        le = LabelEncoder()\n",
        "        # Fit on the combined unique values from both train and test\n",
        "        le.fit(pd.concat([X_train[col], test[col]]).astype(str).unique())\n",
        "\n",
        "        X_train[col] = le.transform(X_train[col].astype(str))\n",
        "        test[col] = le.transform(test[col].astype(str))\n",
        "\n",
        "    # ... (train model, make predictions, etc.)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tjQC0xxJYVCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PID,Gap\n",
        "ID_XXX_N, -14.0\n",
        "ID_XXX_P, 2.1\n",
        "...\n"
      ],
      "metadata": {
        "id": "K56tSqiQYYCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = [['ID_XXX_N', -14.0], ['ID_XXX_P', 2.1]]  # Add more data as needed\n",
        "df = pd.DataFrame(data, columns=['PID', 'Gap'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qxcXWcm6Yrf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "otV6yy7VZl2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After model.fit(...)\n",
        "importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title(f\"Feature Importance for {nutrient}\")\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "huwNcPjuckvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After model.fit(...)\n",
        "importances = model.feature_importances_  # Access feature_importances_ directly\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title(f\"Feature Importance for {nutrient}\")\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NMWVJ3lIct8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the model has been trained before accessing feature importances\n",
        "model.fit(X_train, y_train)  # Assuming X_train and y_train are defined\n",
        "\n",
        "# Now access feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title(f\"Feature Importance for {nutrient}\")\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aRugy9oGc1Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate model with 5-fold cross-validation\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "print(f\"Cross-Validation RMSE for {nutrient}: {-np.mean(scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "cDqr7n1edAwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Amini Soil Prediction Challenge – Zindi\n",
        "\n",
        "## 👤 Author\n",
        "James Mukoma Mburu (`@jmsmuigai` on Zindi)\n",
        "\n",
        "## 🎯 Goal\n",
        "To predict soil nutrient gaps for 11 essential nutrients to support maize yield (target = 4 tons/ha).\n",
        "# Amini Soil Prediction Challenge – Zindi\n",
        "\n",
        "## 👤 Author\n",
        "James Mukoma Mburu (`@jmsmuigai` on Zindi)\n",
        "\n",
        "## 🎯 Goal\n",
        "To predict soil nutrient gaps for 11 essential nutrients to support maize yield (target = 4 tons/ha).\n",
        "# Amini Soil Prediction Challenge – Zindi\n",
        "\n",
        "## 👤 Author\n",
        "James Mukoma Mburu (`@jmsmuigai` on Zindi)\n",
        "\n",
        "## 🎯 Goal\n",
        "To predict soil nutrient gaps for 11 essential nutrients to support maize yield (target = 4 tons/ha).\n",
        "# Amini Soil Prediction Challenge – Zindi\n",
        "\n",
        "## 👤 Author\n",
        "James Mukoma Mburu (`@jmsmuigai` on Zindi)\n",
        "\n",
        "## 🎯 Goal\n",
        "To predict soil nutrient gaps for 11 essential nutrients to support maize yield (target = 4 tons/ha).\n",
        "\n",
        "## 📁 Files\n",
        "- `Train.csv` – includes features + nutrient availability\n",
        "- `Test.csv` – includes only features (predict nutrient gaps)\n",
        "- `SampleSubmission.csv` – format template for submission\n",
        "- `submission.csv` – your model predictions\n",
        "\n",
        "## 🧠 Methodology\n",
        "- Data reshaped to long format with nutrient as key.\n",
        "- `RandomForestRegressor` used for each nutrient.\n",
        "- Predictions saved in submission format for Zindi.\n",
        "\n",
        "## 🧪 Evaluation\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Cross-validation used: 5-fold CV per nutrient\n",
        "\n",
        "## 🧰 Environment\n",
        "- Google Colab\n",
        "- Python 3.11\n",
        "- Libraries: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `seaborn`\n",
        "\n",
        "## 📊 Results\n",
        "- Feature importance visualized for model interpretability\n",
        "- Average RMSE across nutrients: TBD\n",
        "\n",
        "## 📌 How to Run\n",
        "1. Open the notebook in Colab\n",
        "2. Upload the 3 CSVs\n",
        "3. Run all cells\n",
        "4. Download `submission.csv`\n",
        "5. Upload to Zindi\n",
        "\n",
        "## ⚠️ Notes\n",
        "- Only open-source libraries used\n",
        "- Follows Zindi rules: no external data, no AutoML\n",
        "- `submission.csv` – your model predictions\n",
        "\n",
        "## 🧠 Methodology\n",
        "- Data reshaped to long format with nutrient as key.\n",
        "- `RandomForestRegressor` used for each nutrient.\n",
        "- Predictions saved in submission format for Zindi.\n",
        "\n",
        "## 🧪 Evaluation\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Cross-validation used: 5-fold CV per nutrient\n",
        "\n",
        "## 🧰 Environment\n",
        "- Google Colab\n",
        "- Python 3.11\n",
        "- Libraries: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `seaborn`\n",
        "\n",
        "## 📊 Results\n",
        "- Feature importance visualized for model interpretability\n",
        "- Average RMSE across nutrients: TBD\n",
        "\n",
        "## 📌 How to Run\n",
        "1. Open the notebook in Colab\n",
        "2. Upload the 3 CSVs\n",
        "3. Run all cells\n",
        "4. Download `submission.csv`\n",
        "5. Upload to Zindi\n",
        "\n",
        "## ⚠️ Notes\n",
        "- Only open-source libraries used\n",
        "- Follows Zindi rules: no external data, no AutoML\n",
        "- `submission.csv` – your model predictions\n",
        "\n",
        "## 🧠 Methodology\n",
        "- Data reshaped to long format with nutrient as key.\n",
        "- `RandomForestRegressor` used for each nutrient.\n",
        "- Predictions saved in submission format for Zindi.\n",
        "\n",
        "## 🧪 Evaluation\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Cross-validation used: 5-fold CV per nutrient\n",
        "\n",
        "## 🧰 Environment\n",
        "- Google Colab\n",
        "- Python 3.11\n",
        "- Libraries: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `seaborn`\n",
        "\n",
        "## 📊 Results\n",
        "- Feature importance visualized for model interpretability\n",
        "- Average RMSE across nutrients: TBD\n",
        "\n",
        "## 📌 How to Run\n",
        "1. Open the notebook in Colab\n",
        "2. Upload the 3 CSVs\n",
        "3. Run all cells\n",
        "4. Download `submission.csv`\n",
        "5. Upload to Zindi\n",
        "\n",
        "## ⚠️ Notes\n",
        "- Only open-source libraries used\n",
        "- Follows Zindi rules: no external data, no AutoML\n",
        "- `submission.csv` – your model predictions\n",
        "\n",
        "## 🧠 Methodology\n",
        "- Data reshaped to long format with nutrient as key.\n",
        "- `RandomForestRegressor` used for each nutrient.\n",
        "- Predictions saved in submission format for Zindi.\n",
        "\n",
        "## 🧪 Evaluation\n",
        "- Root Mean Squared Error (RMSE)\n",
        "- Cross-validation used: 5-fold CV per nutrient\n",
        "\n",
        "## 🧰 Environment\n",
        "- Google Colab\n",
        "- Python 3.11\n",
        "- Libraries: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `seaborn`\n",
        "\n",
        "## 📊 Results\n",
        "- Feature importance visualized for model interpretability\n",
        "- Average RMSE across nutrients: TBD\n",
        "\n",
        "## 📌 How to Run\n",
        "1. Open the notebook in Colab\n",
        "2. Upload the 3 CSVs\n",
        "3. Run all cells\n",
        "4. Download `submission.csv`\n",
        "5. Upload to Zindi\n",
        "\n",
        "## ⚠️ Notes\n",
        "- Only open-source libraries used\n",
        "- Follows Zindi rules: no external data, no AutoML\n"
      ],
      "metadata": {
        "id": "pC4sA897d86n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No code provided to correct.  Assuming this is a comment in a notebook cell.\n",
        "# – Train.csv – includes features + nutrient availability"
      ],
      "metadata": {
        "id": "hi11s-5ke1jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No code to correct; this appears to be a comment."
      ],
      "metadata": {
        "id": "48iiwN6Pe55c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Train.csv` - includes features + nutrient availability\n",
        "- `Test.csv` - includes features only (predict nutrient gaps)\n",
        "- `SampleSubmission.csv` - submission template\n",
        "- `submission.csv` - your predictions for Zindi\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XzfRgcU6fokh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate with cross-validation\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
        "print(f\"Cross-Validation RMSE for {nutrient}: {-np.mean(scores):.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LN6HcUeWgMpo"
      }
    },
    {
      "source": [
        "# Task\n",
        "here are the files automatically add them where needed and resolve this issue\n",
        "\n",
        "Here is all the data you need:\n",
        "\"Train.csv\"\n",
        "\"Test.csv\"\n",
        "\"SampleSubmission.csv\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YOsarayR2n5H"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the provided CSV files into pandas DataFrames.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "L6bEBjkX2oI7"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Load the three CSV files into pandas DataFrames.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "osZg1Yxy2oYo"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('Train.csv')\n",
        "    test_df = pd.read_csv('Test.csv')\n",
        "    submission_df = pd.read_csv('SampleSubmission.csv')\n",
        "    display(train_df.head())\n",
        "    display(test_df.head())\n",
        "    display(submission_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or more CSV files not found.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error: Could not parse one or more CSV files.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sLmKCDxW2ooZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded datasets (`train_df`, `test_df`, and `submission_df`) to understand their structure, identify potential issues, and gain initial insights.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEU5PGJ2tA9"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Explore the dataframes to understand their structure, identify potential issues, and gain initial insights by examining their shape, data types, missing values, descriptive statistics, distributions of key features, correlation analysis, and unique values in categorical columns. Also analyze the submission dataframe to understand the required format for prediction output.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "NRxzNGWV2uii"
      }
    },
    {
      "source": [
        "# Examine the shape of each DataFrame\n",
        "print(\"Train DataFrame shape:\", train_df.shape)\n",
        "print(\"Test DataFrame shape:\", test_df.shape)\n",
        "print(\"Submission DataFrame shape:\", submission_df.shape)\n",
        "\n",
        "# Check data types and missing values\n",
        "print(\"\\nTrain DataFrame info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nTest DataFrame info:\")\n",
        "print(test_df.info())\n",
        "print(\"\\nSubmission DataFrame info:\")\n",
        "print(submission_df.info())\n",
        "\n",
        "print(\"\\nTrain DataFrame missing values:\")\n",
        "print(train_df.isnull().sum())\n",
        "print(\"\\nTest DataFrame missing values:\")\n",
        "print(test_df.isnull().sum())\n",
        "print(\"\\nSubmission DataFrame missing values:\")\n",
        "print(submission_df.isnull().sum())\n",
        "\n",
        "# Descriptive statistics for numerical features\n",
        "print(\"\\nTrain DataFrame descriptive statistics:\")\n",
        "print(train_df.describe())\n",
        "print(\"\\nTest DataFrame descriptive statistics:\")\n",
        "print(test_df.describe())\n",
        "\n",
        "# Distribution of key features (example: 'pH')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(train_df['pH'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('pH Distribution (Train)')\n",
        "plt.xlabel('pH')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(test_df['pH'], bins=20, color='salmon', edgecolor='black')\n",
        "plt.title('pH Distribution (Test)')\n",
        "plt.xlabel('pH')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis for numerical features in train_df\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = train_df.corr()\n",
        "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Unique values in categorical columns (example: 'site')\n",
        "print(\"\\nUnique values in 'site' column (Train):\", train_df['site'].unique())\n",
        "print(\"\\nUnique values in 'site' column (Test):\", test_df['site'].unique())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xblE8TkE2uyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed due to an error in calculating the correlation matrix because of non-numeric data in the dataframe.  I'll try again, but this time I will specify `numeric_only=True` in the `.corr()` method to address the error.  I will also remove the correlation matrix visualization since that depends on the successful generation of the matrix. I will also remove unique value analysis since this is not part of the immediate task, and it's not clear how to proceed with this information. I will also focus on the data cleaning steps mentioned in the original instructions.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "UISdWxkN2wy1"
      }
    },
    {
      "source": [
        "# Examine the shape of each DataFrame\n",
        "print(\"Train DataFrame shape:\", train_df.shape)\n",
        "print(\"Test DataFrame shape:\", test_df.shape)\n",
        "print(\"Submission DataFrame shape:\", submission_df.shape)\n",
        "\n",
        "# Check data types and missing values\n",
        "print(\"\\nTrain DataFrame info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nTest DataFrame info:\")\n",
        "print(test_df.info())\n",
        "print(\"\\nSubmission DataFrame info:\")\n",
        "print(submission_df.info())\n",
        "\n",
        "print(\"\\nTrain DataFrame missing values:\")\n",
        "print(train_df.isnull().sum())\n",
        "print(\"\\nTest DataFrame missing values:\")\n",
        "print(test_df.isnull().sum())\n",
        "print(\"\\nSubmission DataFrame missing values:\")\n",
        "print(submission_df.isnull().sum())\n",
        "\n",
        "# Descriptive statistics for numerical features\n",
        "print(\"\\nTrain DataFrame descriptive statistics:\")\n",
        "print(train_df.describe())\n",
        "print(\"\\nTest DataFrame descriptive statistics:\")\n",
        "print(test_df.describe())\n",
        "\n",
        "# Distribution of key features (example: 'pH')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(train_df['pH'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('pH Distribution (Train)')\n",
        "plt.xlabel('pH')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(test_df['pH'], bins=20, color='salmon', edgecolor='black')\n",
        "plt.title('pH Distribution (Test)')\n",
        "plt.xlabel('pH')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis for numerical features in train_df\n",
        "correlation_matrix = train_df.corr(numeric_only=True)\n",
        "print(correlation_matrix)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Qx4HxJDL2xCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data cleaning\n",
        "\n",
        "### Subtask:\n",
        "Clean the `train_df` and `test_df` DataFrames.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "aEQ6Lz-h22Bu"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Impute missing values in 'ecec20', 'hp20', 'xhp20', and 'BulkDensity' columns of `train_df` using the median, then ensure data type consistency and remove duplicates in both dataframes.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GbLqmsjU225L"
      }
    },
    {
      "source": [
        "# Impute missing values in train_df using the median\n",
        "for col in ['ecec20', 'hp20', 'xhp20', 'BulkDensity']:\n",
        "    median_val = train_df[col].median()\n",
        "    train_df[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# Ensure data type consistency\n",
        "numerical_cols = ['pH', 'alb', 'bio1', 'bio12', 'bio15', 'bio7', 'bp', 'cec20', 'dows', 'ecec20', 'hp20', 'ls', 'lstd', 'lstn', 'mb1', 'mb2', 'mb3', 'mb7', 'mdem', 'para', 'parv', 'ph20', 'slope', 'snd20', 'soc20', 'tim', 'wp', 'xhp20', 'BulkDensity', 'N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "for col in numerical_cols:\n",
        "    if col in train_df.columns:\n",
        "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "    if col in test_df.columns:\n",
        "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
        "\n",
        "# Remove duplicates\n",
        "train_duplicates = train_df.duplicated().sum()\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "test_duplicates = test_df.duplicated().sum()\n",
        "test_df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(f\"Removed {train_duplicates} duplicates from train_df\")\n",
        "print(f\"Removed {test_duplicates} duplicates from test_df\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1vtZA7KD23I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code block had a warning about chained assignment.  I will fix this and re-run the imputation step.  Then, I will proceed to outlier handling, using IQR to identify and cap outliers. I will apply the same outlier handling to both dataframes.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "l3X3B9P6245H"
      }
    },
    {
      "source": [
        "# Fix imputation with correct method\n",
        "for col in ['ecec20', 'hp20', 'xhp20', 'BulkDensity']:\n",
        "    median_val = train_df[col].median()\n",
        "    train_df[col] = train_df[col].fillna(median_val)\n",
        "\n",
        "# Outlier handling using IQR\n",
        "def handle_outliers_iqr(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[col] = df[col].clip(lower_bound, upper_bound)\n",
        "    return df\n",
        "\n",
        "numerical_cols = ['pH', 'alb', 'bio1', 'bio12', 'bio15', 'bio7', 'bp', 'cec20', 'dows', 'ecec20', 'hp20', 'ls', 'lstd', 'lstn', 'mb1', 'mb2', 'mb3', 'mb7', 'mdem', 'para', 'parv', 'ph20', 'slope', 'snd20', 'soc20', 'tim', 'wp', 'xhp20', 'BulkDensity', 'N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if col in train_df.columns and pd.api.types.is_numeric_dtype(train_df[col]):\n",
        "        train_df = handle_outliers_iqr(train_df, col)\n",
        "    if col in test_df.columns and pd.api.types.is_numeric_dtype(test_df[col]):\n",
        "        test_df = handle_outliers_iqr(test_df, col)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9MNw0GHB25I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Engineer new features from the existing ones in `train_df` and apply the same transformations to `test_df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wMS7aRlu29ON"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Create interaction, polynomial, ratio, and log transformation features for both train and test datasets. Handle potential errors during log transformation.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "dWdcnDPG2-F2"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Interaction Features\n",
        "interaction_features = ['pH', 'BulkDensity', 'N', 'P', 'K', 'Ca', 'Mg', 'S']\n",
        "for i in range(len(interaction_features)):\n",
        "    for j in range(i + 1, len(interaction_features)):\n",
        "        feature1 = interaction_features[i]\n",
        "        feature2 = interaction_features[j]\n",
        "        new_feature_name = f'{feature1}_{feature2}_interaction'\n",
        "        train_df[new_feature_name] = train_df[feature1] * train_df[feature2]\n",
        "        test_df[new_feature_name] = test_df[feature1] * test_df[feature2]\n",
        "\n",
        "# Polynomial Features\n",
        "polynomial_features = ['P', 'K', 'Ca', 'Mg']\n",
        "for feature in polynomial_features:\n",
        "    train_df[f'{feature}_squared'] = train_df[feature]**2\n",
        "    test_df[f'{feature}_squared'] = test_df[feature]**2\n",
        "\n",
        "# Ratio Features\n",
        "ratio_features = ['N', 'P']\n",
        "for i in range(len(ratio_features)):\n",
        "    for j in range(i + 1, len(ratio_features)):\n",
        "      feature1 = ratio_features[i]\n",
        "      feature2 = ratio_features[j]\n",
        "      new_feature_name = f'{feature1}_to_{feature2}_ratio'\n",
        "      train_df[new_feature_name] = train_df[feature1] / train_df[feature2]\n",
        "      test_df[new_feature_name] = test_df[feature1] / test_df[feature2]\n",
        "\n",
        "# Log Transformation\n",
        "log_transform_features = ['bio1', 'bio7', 'soc20']\n",
        "for feature in log_transform_features:\n",
        "    # Add a small constant to handle potential zeros or negative values\n",
        "    train_df[f'{feature}_log'] = np.log1p(train_df[feature])\n",
        "    test_df[f'{feature}_log'] = np.log1p(test_df[feature])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tHKefnZz2-Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The error message indicates that the column 'N' is not present in the test_df.  The code needs to be modified to check for the existence of the column before creating the interaction features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ihn3Sy23Aur"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Interaction Features\n",
        "interaction_features = ['pH', 'BulkDensity'] # Removed 'N' as it's not in test_df\n",
        "for i in range(len(interaction_features)):\n",
        "    for j in range(i + 1, len(interaction_features)):\n",
        "        feature1 = interaction_features[i]\n",
        "        feature2 = interaction_features[j]\n",
        "        new_feature_name = f'{feature1}_{feature2}_interaction'\n",
        "        if feature1 in train_df.columns and feature2 in train_df.columns:\n",
        "            train_df[new_feature_name] = train_df[feature1] * train_df[feature2]\n",
        "        if feature1 in test_df.columns and feature2 in test_df.columns:\n",
        "            test_df[new_feature_name] = test_df[feature1] * test_df[feature2]\n",
        "\n",
        "# Polynomial Features\n",
        "polynomial_features = ['P', 'K', 'Ca', 'Mg']\n",
        "for feature in polynomial_features:\n",
        "    if feature in train_df.columns:\n",
        "        train_df[f'{feature}_squared'] = train_df[feature]**2\n",
        "    if feature in test_df.columns:\n",
        "        test_df[f'{feature}_squared'] = test_df[feature]**2\n",
        "\n",
        "# Ratio Features\n",
        "ratio_features = ['P', 'K'] # Removed 'N'\n",
        "for i in range(len(ratio_features)):\n",
        "    for j in range(i + 1, len(ratio_features)):\n",
        "        feature1 = ratio_features[i]\n",
        "        feature2 = ratio_features[j]\n",
        "        new_feature_name = f'{feature1}_to_{feature2}_ratio'\n",
        "        if feature1 in train_df.columns and feature2 in train_df.columns:\n",
        "            train_df[new_feature_name] = train_df[feature1] / train_df[feature2]\n",
        "        if feature1 in test_df.columns and feature2 in test_df.columns:\n",
        "            test_df[new_feature_name] = test_df[feature1] / test_df[feature2]\n",
        "\n",
        "# Log Transformation\n",
        "log_transform_features = ['bio1', 'bio7', 'soc20']\n",
        "for feature in log_transform_features:\n",
        "    if feature in train_df.columns:\n",
        "        train_df[f'{feature}_log'] = np.log1p(train_df[feature])\n",
        "    if feature in test_df.columns:\n",
        "        test_df[f'{feature}_log'] = np.log1p(test_df[feature])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mlq0c-XC3BAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data splitting\n",
        "\n",
        "### Subtask:\n",
        "Split the `train_df` DataFrame into training and validation sets.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uSJpr_553Ezt"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Split the `train_df` DataFrame into training and validation sets using `train_test_split`.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z8ulLkS3GBk"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the target variable (y)\n",
        "target_column = 'N'  # Based on the initial data exploration and submission file\n",
        "y = train_df[target_column]\n",
        "\n",
        "# Define the features (X)\n",
        "X = train_df.drop(columns=[target_column, 'site', 'PID'])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I3tIdGPD3GYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the target variable 'N' has classes with only one member, which prevents stratification.  I will try to use a different target variable or remove stratification.  Given the submission file's target variable name is 'Gap', I will try using 'Gap' instead of 'N'.  Since 'Gap' is not in `train_df`, I will remove stratification entirely to proceed.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "R48AtlD13Itx"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the target variable (y)\n",
        "target_column = 'Gap' # Try using 'Gap' from the SampleSubmission.csv\n",
        "# Since 'Gap' is not present in train_df, remove stratification\n",
        "try:\n",
        "    y = train_df[target_column]\n",
        "    X = train_df.drop(columns=[target_column, 'site', 'PID'])\n",
        "\n",
        "    # Split data into training and validation sets without stratification\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "except KeyError:\n",
        "    print(f\"Error: Target column '{target_column}' not found in train_df. Using 'N' as target and removing stratification.\")\n",
        "    target_column = 'N'\n",
        "    y = train_df[target_column]\n",
        "    X = train_df.drop(columns=[target_column, 'site', 'PID'])\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OCkeufNR3JEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a RandomForestRegressor model on the prepared training data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "rmciGu3Z3NmQ"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Train a RandomForestRegressor model using the prepared training data (X_train, y_train).\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wOjcXfXb3N8z"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the model with random_state for reproducibility\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g9mUva4r3OcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model optimization\n",
        "\n",
        "### Subtask:\n",
        "Optimize the hyperparameters of the trained RandomForestRegressor model using RandomizedSearchCV.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "T-o8Fc6T3aig"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Optimize the hyperparameters of the RandomForestRegressor model using RandomizedSearchCV as instructed.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "qqNEnHgG3b0o"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10, 15],\n",
        "    'min_samples_leaf': [1, 2, 4, 8]\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=10,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object\n",
        "random_search.fit(X_val, y_val)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf_model = random_search.best_estimator_"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkV2I7Si3cHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the best RandomForestRegressor model (`best_rf_model`) on the validation set (`X_val`, `y_val`)\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "EmQISWY9412v"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the best RandomForestRegressor model on the validation set.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCJ0al942zC"
      }
    },
    {
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = best_rf_model.predict(X_val)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aRu1RrzN43MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the final RandomForestRegressor model using the optimized hyperparameters and the entire training dataset (`X`, `y`)\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ukDdmc47uy"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Train the final RandomForestRegressor model using the best hyperparameters found in the previous step and the entire training dataset (X, y).\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ql2BG4Jk488E"
      }
    },
    {
      "source": [
        "# Train the final model using the best hyperparameters and the entire training dataset\n",
        "final_model = best_rf_model.fit(X, y)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IN-VJR8749Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Prepare the `test_df` DataFrame for prediction using the final trained model.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "HjWO1bAj5aAO"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Prepare the test data (`test_df`) for prediction by aligning its features with the training data used to train the final model (`final_model`).  This includes handling missing columns and applying the same transformations used during training.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ThBDwCQ05bM_"
      }
    },
    {
      "source": [
        "# Ensure test_df has the same columns as X_train\n",
        "missing_cols = set(X_train.columns) - set(test_df.columns)\n",
        "for col in missing_cols:\n",
        "    test_df[col] = 0  # Fill missing columns with 0 in test_df\n",
        "\n",
        "# Align column order\n",
        "X_test = test_df[X_train.columns]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5Nu4b5F35bkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Make predictions on the prepared test data (`X_test`) using the final trained model (`final_model`)\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "pXfZnAn45f2v"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Make predictions on the prepared test data (X_test) using the final trained model (final_model) and print the first 10 predictions.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "amXTgS0G5gM9"
      }
    },
    {
      "source": [
        "# Predict on the test set\n",
        "predictions = final_model.predict(X_test)\n",
        "\n",
        "# Print the first 10 predictions\n",
        "print(predictions[:10])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7eXLDkEg5gsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Format the predictions to match the format of `submission_df`.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ezYrK8ti5ltw"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Create a submission dataframe with the same structure as `submission_df`, populate it with predictions and ID, and save it to a csv file.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkg7NJ255mD8"
      }
    },
    {
      "source": [
        "# Create a submission DataFrame\n",
        "submission = pd.DataFrame({'ID': submission_df['ID'], 'Gap': predictions})\n",
        "\n",
        "# Save the submission DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zSmd5AIp5mTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'X_test' was derived from 'df_test' and 'submission_df' from 'sample'\n",
        "predictions = final_model.predict(df_test.drop(columns=['ID'])) # Predict on the entire test data\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission = pd.DataFrame({'ID': sample['ID'], 'Gap': predictions})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VZvp3WA759el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'X_test' was derived from 'df_test' and 'submission_df' from 'sample'\n",
        "# Assuming you have already loaded your test data into a DataFrame called 'df_test'\n",
        "# If not, load it using pd.read_csv('your_test_data_file.csv')\n",
        "predictions = final_model.predict(X_test)  # Use X_test if it's the preprocessed test data\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission = pd.DataFrame({'ID': submission_df['ID'], 'Gap': predictions})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wa26dKlU6GGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'X_test' was derived from 'df_test' and 'submission_df' from 'sample'\n",
        "# Assuming you have already loaded your test data into a DataFrame called 'df_test'\n",
        "# If not, load it using pd.read_csv('your_test_data_file.csv')\n",
        "predictions = final_model.predict(X_test)  # Use X_test if it's the preprocessed test data\n",
        "\n",
        "# Create a submission DataFrame\n",
        "# Use the length of 'predictions' to slice 'submission_df'\n",
        "submission = pd.DataFrame({'ID': submission_df['ID'][:len(predictions)], 'Gap': predictions})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9_3BYAoC6Prc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Summary:\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "* **What is the target variable used for model training and evaluation?** Initially, 'N' was used, but due to issues with stratification, the code reverted to using 'N' without stratification.  The 'Gap' column from `SampleSubmission.csv` was attempted but not found in the training data.\n",
        "\n",
        "* **What is the performance of the best RandomForestRegressor model on the validation set?** The model achieved an MSE of 717.57, an RMSE of 26.79, and an R-squared of 0.9985.\n",
        "\n",
        "* **Why did the submission file creation fail?** The submission file creation failed because the number of predictions (2418) did not match the number of IDs in the `submission_df` (26598). This indicates that predictions were not generated for all entries in the test dataset.\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "* **Data Imbalance:** The initial attempt to use 'N' as the target variable and stratify the data split failed due to some classes in the target variable having only one member.\n",
        "* **Missing Values:**  `train_df` had missing values in columns 'ecec20', 'hp20', 'xhp20', and 'BulkDensity' which were imputed using the median.\n",
        "* **Outlier Handling:** Outliers in numerical features were handled using the IQR method in both `train_df` and `test_df`.\n",
        "* **Feature Engineering:** New features were created through interaction, polynomial transformation, ratio calculation, and log transformation.\n",
        "* **Model Performance:** The optimized RandomForestRegressor achieved a high R-squared (0.9985) and low RMSE (26.79) on the validation set.\n",
        "* **Prediction Mismatch:** The number of predictions generated (2418) did not match the number of IDs in the submission file (26598).\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "* **Investigate the prediction mismatch:** The discrepancy between the number of predictions and the number of IDs in the submission file needs to be investigated and resolved.  The test data preparation and prediction steps need to be carefully reviewed to ensure that all test samples are processed correctly.  It seems like the `test_df` has less samples than the `submission_df`\n",
        "* **Explore alternative models or feature engineering:** While the RandomForestRegressor performed well, exploring different models (e.g., gradient boosting, neural networks) or alternative feature engineering techniques could potentially improve the results and address the prediction mismatch.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "4GwsxgnI5rVl"
      }
    }
  ]
}